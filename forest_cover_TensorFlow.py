# -*- coding: utf-8 -*-
"""Forest_Cover_2019.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ARiJPVlzuxZNyVSe0KOXCxGQlTnrr9W
"""

import pandas as pd
import tensorflow as tf
import matplotlib as plt

from tensorflow.keras import layers, optimizers
from tensorflow.metrics import recall, accuracy, recall
from tensorflow.contrib.metrics import f1_score
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import f1_score, accuracy_score, log_loss, recall_score, precision_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import sklearn.utils
import time 
import numpy as np


def standardize (train):
    """
    given a forest cover type dataset, this function transform the data
    to StandardScaler [-1,1] scale
    Returns just first 10 atributes on tha scale concatenate with the other
    atributes
    """

    train1 = train

    Size = 10

    X_temp1 = train1.iloc[:, :Size]

    # Transform to StandardScaler
    X_temp1 = StandardScaler().fit_transform(X_temp1)

    r, c = train.shape
    X_train1 = np.concatenate((X_temp1, train1.iloc[:, Size:c - 1]), axis=1)  

    return X_train1

def compare_classifiers (optim, dataset_train, dataset_validation, dataset_test):

  log_cols = ["Accuracy", "Precission", "Recall", "F1 Score", "Log Loss", "Time"]
  log = pd.DataFrame(columns=log_cols)
  
  model = tf.keras.Sequential([
  layers.Dense(1024, activation = 'relu'),
  layers.Dense(512, activation = 'relu'),
  layers.Dense(256, activation = 'relu'),
  layers.Dense(128, activation = 'relu'),
  layers.Dense(N_CLASSES, activation = 'softmax'),
  ])
   
  
  for x in optim :
    print("=" * 30)
    print ("OPTIMIZER ----> : ", x)
    start = time.time()
    model.compile(optimizer = x, loss ='categorical_crossentropy', metrics=['accuracy'])
    model.fit(dataset_train, epochs=10, validation_data=dataset_validation)
    time_spent = (time.time() - start)
    
    
    
    model.evaluate(dataset_validation)
    y_pred = model.predict(dataset_test)
    y_pred_one_hot = []
    for prediction in y_pred:
      max_value = np.argmax(prediction)
      one_hot_vector = np.zeros(N_CLASSES)
      one_hot_vector[max_value] = 1
      y_pred_one_hot.append(one_hot_vector)
  
    y_pred_one_hot = np.array(y_pred_one_hot)
    
    
    acc = accuracy_score(Y_test, y_pred_one_hot)
    acc = acc * 100
    rec = recall_score(Y_test, y_pred_one_hot, average = 'weighted')
    rec = rec * 100
    prec = precision_score(Y_test, y_pred_one_hot, average = 'weighted')
    prec = prec * 100
    f1 = f1_score(Y_test, y_pred_one_hot, average = 'weighted')
    f1 = f1 * 100
    ll = log_loss(Y_test, y_pred_one_hot)
    entry = pd.DataFrame([[acc, prec, rec, f1, ll, time_spent]], columns=log_cols)
    log = log.append(entry)
    print(" took %.2f seconds to fit" % (time_spent))
    print("Accuracy of the classifier: %.2f"%(acc),"%")
    print("Precision score of the classifier: %.2f"%(prec),"%")
    print("Recall of the classifier: %.2f"%(rec),"%")
    print("F1 Score of the classifier : %.2f"%(f1),"%")
    class_report = classification_report(Y_test, y_pred_one_hot)
    print ("Classification report: ")
    print(class_report)
    print("=" * 30)
  
  return log



!wget https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz
!gzip -d "covtype.data.gz"


df2 = pd.read_csv("covtype.data", header=None) 
df1 = sklearn.utils.shuffle(df2)
df = sklearn.utils.shuffle(df1)

N_CLASSES = 7
TRAIN_SIZE = int(0.7 * df.shape[0])
TEST_SIZE = int(0.15 * df.shape[0])
VALIDATION_SIZE = int(0.15 * df.shape[0])

features = standardize(df.iloc[:, 0:54])
#features = df.iloc[:, 0:54]
labels = df.iloc[:, 54:].values

one_hot_encoded = []
  
for label in labels:
  row = np.zeros(N_CLASSES)
  row[label - 1] = 1
  one_hot_encoded.append(row)
  
y_one_hot = np.array(one_hot_encoded)

X_train, Y_train = features[:TRAIN_SIZE], y_one_hot[:TRAIN_SIZE]
X_test, Y_test = features[TRAIN_SIZE: TRAIN_SIZE + TEST_SIZE], y_one_hot[TRAIN_SIZE: TRAIN_SIZE + TEST_SIZE]
X_validation, Y_validation = features[TRAIN_SIZE + TEST_SIZE : ], y_one_hot[TRAIN_SIZE + TEST_SIZE : ]

  
dataset_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(256).shuffle(buffer_size=1000)
dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(256)
dataset_validation = tf.data.Dataset.from_tensor_slices((X_validation, Y_validation)).batch(256)


sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
rms= optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)
ada= optimizers.Adagrad(lr=0.001, epsilon=None, decay=0.0)
delta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)
adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
adamax = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)
nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)

optimizers = [adam, sgd, rms, ada, delta, adamax, nadam]

log = compare_classifiers(optimizers, dataset_train, dataset_validation, dataset_test)
print (log)
log.to_csv('tensorflow.csv', mode='a', sep=',')

